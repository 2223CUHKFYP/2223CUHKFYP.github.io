<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2023 FYP Report</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="style.css">

</head>

<body>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <a class="navbar-brand" href="index.html" id="nor_button">PAH2203 Final Year Project Report: Semantic
                    Understanding in Haze Conditions with Video Data<br>
                    <b style="font-size:medium">Ng Hon Lam 1155143298, CUHK<br>
                        Lam Shi Shing 1155142786, CUHK
                </a>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

                <ul class="nav navbar-nav">
                    <li class="nav_button"><a href="index.html">Overview</a></li>
                    <li class="nav_button"><a href="dataset.html">Dataset</a></li>
                    <li class="nav_button"><a href="haze.html">Hazing &amp; Dehazing</a></li>
                    <li class="nav_button"><a href="mot.html">MOT &amp; MOTS</a></li>
                    <li class="nav_button"><a href="experiment.html">Experiment &amp; Result</a></li>
                </ul>
            </div>
        </div><!-- /.container-fluid -->
    </nav>

    <!-- Main Content -->


    <div class="container">
        <div style="margin-top:10rem;">
            <h1 class="text-center mb-4">MOT &amp; MOTS</h1>
        </div>

        <!-- Navigation Bar -->
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                        data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>


            </div><!-- /.container-fluid -->
        </nav>
        <div class="row">
            <h2 class="p_title">Task</h2>
            <div class="col-md-6 mb-4">
                <h2 class="p_subtitle">MOT</h2>
                <p class="p_text">
                    Multiple Object Tracking (MOT) involves detecting and tracking objects of interest throughout each video sequence.<br>
                    The below picture shows an example:
                </p>
                <figure>
                    <img src="resources/box_track1.gif" alt="Example" class="img_center">
                    <figcaption class="img_caption">Example BDD100K MOT. Referenced in <a href="https://github.com/SysCV/bdd100k-models/tree/main/mot">MOT</a></figcaption>
                </figure>
            </div>
            <div class="col-md-6 mb-4">
                <h2 class="p_subtitle">MOTS</h2>
                <p class="p_text">
                    Multiple Object Tracking and Segmentation (MOTS) involves detecting, tracking, and segmenting objects of interest throughout each video sequence.<br>
                    The below picture shows an example:
                </p>
                <figure>
                    <img src="resources/seg_track1.gif" alt="Example" class="img_center">
                    <figcaption class="img_caption">Example BDD100K MOTS. Referenced in <a href="https://github.com/SysCV/bdd100k-models/tree/main/mots">MOTS</a></figcaption>
                </figure>
            </div>
        </div>
        <hr />
        <div class="row">
            <h2 class="p_title">Model</h2>
            <div class="col-md-6 mb-4">
                <h2 class="p_subtitle">PCAN</h2>
                    Prototypical Cross-Attention Networks (PCAN) is used to perform MOTS. It is also the baseline model provided by BDD100K official. 
                    The model is capable of leveraging rich spatio-temporal information for online multiple object tracking and segmentation. 
                    PCAN first distills a space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. 
                    To segment each object, PCAN adopts a prototypical appearance module to learn a set of contrastive foreground and background prototypes, which are then propagated over time.
                <figure>
                    <img src="resources/pcan-banner-final.png" alt="Example" class="img_center">
                    <figcaption class="img_caption">PCAN model architecture. Referenced in <a href="https://arxiv.org/abs/2106.11958">PCAN PAPER</a></figcaption>
                </figure>
                <figure>
                    <img src="resources/frame-level prototypical cross-attention.png" alt="Example" class="img_center">
                    <figcaption class="img_caption">Overview of frame-level prototypical cross-attention of PCAN. Referenced in <a href="https://arxiv.org/abs/2106.11958">PCAN PAPER</a></figcaption>
                </figure>
                <br><br>
                <p class="p_text">
                    In the experiments, the pretrained PCAN model with ResNet-50 Detector provided is used for MOTS. The results are similar to the ones suggested in the paper, more deatil
                    are shown in the Experiment&Result section.
                </p>
            </div>
            <div class="col-md-6 mb-4">
                <h2 class="p_subtitle">UNICORN</h2>
                    Unicorn is the model that achieves Unification of Object Tracking, it also accomplishes the great unification of the network architecture and the learning paradigm for four tracking tasks.
                    It takes the reference frame and the current frame as the inputs and produces their visual features by a weight-shared backbone. 
                    Then a feature interaction module is exploited to build pixel-wise correspondence between two frames. Based on the correspondence,
                    a target prior is generated by propagating the reference target to the current frame. Finally, the target prior and the visual features are fused and sent to the
                    detection head to get the tracked objects for all tasks.
                    <br><br>
                    Moreover, Unicorn achieves strong performance in BDD100K MOT and MOTS, surpassing the performance of PCAN.
                <figure>
                    <img src="resources/unicorn.png" alt="Example" class="img_center">
                    <figcaption class="img_caption">Comparison between previous solutions and Unicorn. Referenced in <a href="https://arxiv.org/abs/2207.07078">UNICORN PAPER</a></figcaption>
                </figure>
                <figure>
                    <img src="resources/unicorn_arch.png" alt="Example" class="img_center">
                    <figcaption class="img_caption">Unicorn model architecture. Consisting of three main components: (1) Unified inputs and backbone
                        (2) Unified embedding (3) Unified head. Referenced in <a href="https://arxiv.org/abs/2207.07078">UNICORN PAPER</a></figcaption>
                </figure>
                <br><br>
                <p class="p_text">
                    Unicorn models are used in both MOT and MOTS.
                    In the experiments, the pretrained task-specific UNICORN models are used, as their size is relatively small which can be downloaded and managed efficiently.
                    <br>
                    For instance, unicorn_track_tiny_mot_only is used for MOT and unicorn_track_tiny_mots_only is used for MOTS.
                </p>
            </div>
            
        </div>
        <hr />
        <div class="row">
            <h2 class="p_title">Reference</h2>
            <ul>
                <li class="p_text">Official Site: <a href="https://www.vis.xyz/bdd100k/">BDD100K</a> </li>
                <li class="p_text">BDD100K Model Zoo: <a href="https://github.com/SysCV/bdd100k-models/tree/main/mot">BDD100K MOT</a></li>
                <li class="p_text">BDD100K Model Zoo: <a href="https://github.com/SysCV/bdd100k-models/tree/main/mots">BDD100K MOTS</a></li>
                <li class="p_text">PCAN paper: <a href="https://arxiv.org/abs/2106.11958">Prototypical Cross-Attention Networks (PCAN) for Multiple Object Tracking and Segmentation</a></li>
                <li class="p_text">PCAN repository: <a href="https://github.com/SysCV/pcan">PCAN</a></li>
                <li class="p_text">UNICORN paper: <a href="https://arxiv.org/abs/2207.07078">Towards Grand Unification of Object Tracking</a></li>
                <li class="p_text">UNICORN repository: <a href="https://github.com/MasterBin-IIAU/Unicorn">UNICORN</a></li>
            </ul>
        </div>


    </div>


    <!-- jQuery and Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</body>

</html>