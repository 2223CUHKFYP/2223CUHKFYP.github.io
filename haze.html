<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2023 FYP Report</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="style.css">
    <link href="css/twentytwenty.css" rel="stylesheet" type="text/css" />

</head>

<body>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <a class="navbar-brand" href="index.html" id="nor_button">PAH2203 Final Year Project Report: Semantic
                    Understanding in Haze Conditions with Video Data<br>
                    <b style="font-size:medium">Ng Hon Lam 1155143298, CUHK<br>
                        Lam Shi Shing 1155142786, CUHK
                </a>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

                <ul class="nav navbar-nav">
                    <li class="nav_button"><a href="index.html">Overview</a></li>
                    <li class="nav_button"><a href="dataset.html">Dataset</a></li>
                    <li class="nav_button"><a href="haze.html">Hazing &amp; Dehazing</a></li>
                    <li class="nav_button"><a href="mot.html">MOT &amp; MOTS</a></li>
                    <li class="nav_button"><a href="experiment.html">Experiment &amp; Result</a></li>
                </ul>
            </div>
        </div><!-- /.container-fluid -->
    </nav>

    <!-- Main Content -->


    <div class="container">
        <div style="margin-top:10rem;">
            <h1 class="text-center mb-4">Hazing &amp; Dehazing</h1>
        </div>
        <!-- Navigation Bar -->
        <nav class="navbar navbar-default">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                        data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="modified_center">
                    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

                        <ul class="nav navbar-nav">
                            <li class="nav_button"><a href="#home">Home</a></li>
                            <li class="nav_button"><a href="#doc_tool">Doc &amp; Tool</a></li>
                            <li class="nav_button"><a href="#challenges">Challenges</a></li>
                            <li class="nav_button"><a href="#contact">Contact</a></li>
                        </ul>
                    </div>
                </div><!-- /.navbar-collapse -->
            </div><!-- /.container-fluid -->
        </nav>
        <div class="row">
            <div class="col-md-6 mb-4">
                <h2>Introduce</h2>
                <p>This study aims to investigate the efficiency of different dehazing techniques in enhancing semantic
                    understanding in haze conditions with video data. We extract images from the BDD100K dataset and
                    simulate haze conditions using TCMonoDepth for video depth estimation and SeeingThroughFog with
                    varying beta values (0.05, 0.01, 0.02, 0.06) for haze generation. Four prominent dehazing
                    techniques, namely MSBDN, DehazeNet, DehazeNet_indoor, and DCP, are applied to the hazed images, and
                    their performance is compared based on the Structural Similarity Index Measure (SSIM) as an
                    evaluation metric.</p>
                </br>
                <p> this results of this study are expected to provide insights into the most efficive dehazing
                    techniques for video data under hazing condition. It is also helping more powerful computer vision
                    application for challenges environmental condition. The results can also guide future research on
                    optimizing and improving dehazing methods for better semantic understanding under hazy conditions.
                </p>
            </div>

            <div class="col-md-6 mb-4">
                <h1>Techniques</h1>
                <h2>Hazing Techniques</h2>
                <p>Although this study focuses on dehazing techniques, it is important to understand the process of
                    generation synthetic haze in image and video to create a realistic environment for evaluation the
                    performance of dehazing algorithms. The following overview.</p>
                <p>There are two main steps in the process of generating synthetic haze in image and video. The first
                    step is to estimate the depth of the scene. The second step is to generate the haze based on the
                    estimated depth. The following sections will introduce the two steps in detail.</p>
                <ul>
                    <li>TCMonoDepth for Depth Estimation</li>
                    <li>SeeingThroughFog for generate the haze</li>
                </ul>

                <h3>TCMonoDepth for Depth Estimation</h3>
                <p>The first step in simulating haze conditions for images and videos is to estimate the depth of the
                    scene. TCMonoDepth is a method for stable depth estimation for any video.</p>

                <p>In this study, TCMonoDepth is employed to estimate depth information from the images extracted from
                    the BDD100K dataset. These depth maps are then used in conjunction with SeeingThroughFog to generate
                    haze conditions with varying densities, controlled by adjusting the beta parameter.</p>

                <p>By using TCMonoDepth for depth estimation, we ensure that the synthetic haze generation process is
                    more realistic, as it accounts for the spatial distribution of objects in the scene. This approach
                    allows for a more accurate evaluation of the dehazing techniques' performance when applied to video
                    data affected by haze conditions.
                </p>

                <h3>SeeingThroughFog for generate the haze</h3>
                <p>The second step in simulating haze conditions for images and videos is to generate the haze based on
                    the estimated depth. SeeingThroughFog is a method for generating synthetic haze in images and
                    videos.</p>

                <p>In this study, SeeingThroughFog is used in conjunction with TCMonoDepth to generate hazy images from
                    the BDD100K dataset using various beta values (0.05, 0.01, 0.02, 0.06). These hazy images are then
                    processed using different dehazing techniques, and their performance is evaluated using the
                    Structural Similarity Index Measure (SSIM). </p>
            </div>

        </div>
        <div class="row">

            <div class="col-md-6 mb-4">
                <h2>Methodology</h2>
                <h3>Extracting Images from the BDD100K Dataset</h3>
                <p>Introduce the BDD100K <a href="/dataset.html">click me</a>
                </p>

                <h3>Using TCMonoDepth for Video Depth Estimation and SeeingThroughFog for Haze Generation</h3>
                <p>After extraction the images from BDD100K dataset, we generate hazing images by applying TCMonoDepth
                    for video depth estimation and SeeingThroughFog for hazing generation.</p>

                <p>
                    Depth estimation with TCMonoDepth: For each extracted image, we use the TCMonoDepth method to
                    estimate the depth map. This depth map represents the distance between the camera and objects in the
                    scene, providing essential information for simulating realistic haze conditions. The advantage is
                    that TCMonoDepth is trained on a large dataset of synchronized stereo video data to accurately
                    estimate depth maps from single images
                </p>

                <p>
                    Then we use SeeingThroughFog for haze generation. Using the depth map generated by TCMonoDepth, we
                    can apply the SeeingThroughFog method to generate synchronized haze images with varying
                    densities. SeeingThroughFog uses the depth map and atmospheric properties defined by the beta
                    parameter to calculate the attenuation and scattering coefficients of the haze.The final fogged
                    image is produced by combining the input image and the fog layer created using these coefficients
                    with a depth-dependent blending function.
                </p>

                <p>
                    Various beta values: By varying the beta parameter in SeeingThroughFog, we produce hazy photos with
                    various fog densities in order to assess the effectiveness of dehazing procedures under various haze
                    situations. In this study, we represented a variety of haze densities from mild fog to deep fog
                    using four distinct beta values (0.05, 0.01, 0.02, 0.06).

                </p>

                <h4>Results</h4>
                <p>This is some Demo of the Haze Generation by using TCMonoDepth for Video Depth Estimation and
                    SeeingThroughFog for Haze Generation</p>
                <div class="hazingImageCompare">

                    <div class="row">
                        <div class="col-md-6">
                            <div class="demo">
                                <img src="resources/hazingImage/originalImageForBDD100K.jpg" />
                                <img src="resources/hazingImage/hazing0005ImageForBDD100K.jpg" />
                            </div>
                            0.005
                        </div>
                        <div class="col-md-6">
                            <div class="demo">
                                <img src="resources/hazingImage/originalImageForBDD100K.jpg" />
                                <img src="resources/hazingImage/hazing001ImageForBDD100K.jpg" />
                            </div>
                            0.01
                        </div>
                    </div>
                    <div class="row">
                        <div class="col-md-6">
                            <div class="demo">
                                <img src="resources/hazingImage/originalImageForBDD100K.jpg" />
                                <img src="resources/hazingImage/hazing002ImageForBDD100K.jpg" />
                            </div>
                            0.02
                        </div>
                        <div class="col-md-6">
                            <div class="demo">
                                <img src="resources/hazingImage/originalImageForBDD100K.jpg" />
                                <img src="resources/hazingImage/hazing006ImageForBDD100K.jpg" />
                            </div>
                            0.06
                        </div>
                    </div>
                </div>

                </br>
                <p>
                    We generate a realistic and diverse set of hazy images that closely resemble real-world haze
                    conditions by using TCMonoDepth for depth estimation and SeeingThroughFog for haze generation. The
                    different beta values allow us to test the robustness of the dehazing techniques under different
                    haze densities, providing a thorough evaluation of their performance.
                </p>
            </div>
        </div>



    </div>



    <!-- jQuery and Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

    <!-- image compare -->
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <script src="js/jquery.twentytwenty.js"></script>
    <script src="js/jquery.event.move.js"></script>
    <script>
        $(".demo").twentytwenty({

            //  How much of the before image is visible when the page loads
            default_offset_pct: 0.5,

            // or vertical
            orientation: 'horizontal',

            // label text
            before_label: 'Before',
            after_label: 'After',

            // enable/disable overlay
            no_overlay: false,

            // move with handle
            move_with_handle_only: true,

            // click to move
            click_to_move: false

        });  
    </script>
</body>

</html>